# Incremental Learning Research

This repository is dedicated for serving as infrastructure for research in Incremental Learning. The repository hosts all the important literature, code implementations and links to other communities that are dedicated for Incremental Learning research.

If you want to read about Incremental Learning please visit the [literature](https://github.com/BhaskerSriHarsha/Incremental-Learning/blob/master/Literature.md) page. 
If you want to experiement with previously developed models, then visit the [Code Implementations](https://github.com/BhaskerSriHarsha/Incremental-Learning/tree/master/Code%20Implementations) page.

Please note that the repository is still under construction and might miss references to code and papers. All the references will be fixed in due time. 

# What is incremental Learning?

Incremental Learning is also called as Continual learning or Lifelong learning. As the names suggest, it is the ability of a machine learning model to incrementally learn new tasks. Current day neural networks suffer from Catastrophic forgetting when they are tasked with learning new tasks sequentially. Although many architectures have been proposed in the past which help a neural network in remembering previously learnt task to a certain level, the problem still remains open.

# Why is incremental Learning important?

In the quest to realize Artificial General Intelligence, we need to develope Artificial Agents that are capable of learning new things continously in the natural environment. Current day neural network architectures donot serve the purpose as they forget previously learnt task as soon as they learn a new task. So, developing archtiectures that can incrementally learn new tasks is essential for realizing Artificial General Intelligence.

# How can we planning to solve the problem then?

The answer to Incremental Learning can be found in two important questions::

  * Can we mitigate catastrophic forgetting in current Artificial Neural Architecture?
  * Are there any alternate architectures out there that donot suffer from Catastrophic Forgetting?
  
All the currently available techniques try to solve the problem by answering the first question. However, the second question is also of equal importance. Biological Neural Networks seem immune to Catastrophic Forgetting. So, is it possible to build architectures that mimic BNNs to mitigate Catastrophic Forgetting? A strong research effort is needed to answer these questions and this repository is intended to aid the research community by providing knowledge and tools required to tackle the problem.

